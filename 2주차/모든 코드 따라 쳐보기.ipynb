{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "pr-jaunty-yak-21\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"pr-jaunty-yak-21\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anova가 뭐야?\n",
      "ANOVA(분산분석)은 두 개 이상의 모집단 평균에 대한 귀무가설을 검정하는 전체적인 테스트입니다. 가장 간단한 형태의 ANOVA는 한 가지 독립 변수에 의해 분류된 모집단 평균 간의 차이를 테스트하는 일원분산분석(One-Factor ANOVA)입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# 단계 1: 문서 로드(Load Documents)\n",
    "loader = PyMuPDFLoader(\"data\\Statistics 11th Edition_240709_201017.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 단계 2: 문서 분할(Split Documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# 단계 3: 임베딩(Embedding) 생성\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 단계 4: DB 생성(Create DB) 및 저장\n",
    "# 벡터스토어를 생성합니다.\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "\n",
    "# 단계 5: 검색기(Retriever) 생성\n",
    "# 문서에 포함되어 있는 정보를 검색하고 생성합니다.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 단계 6: 프롬프트 생성(Create Prompt)\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean.\n",
    "\n",
    "#Question: \n",
    "{question} \n",
    "#Context: \n",
    "{context} \n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# 단계 7: 언어모델(LLM) 생성\n",
    "# 모델(LLM) 을 생성합니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# 단계 8: 체인(Chain) 생성\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 체인 실행(Run Chain)\n",
    "# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\n",
    "question = \"anova가 뭐야?\"\n",
    "response = chain.invoke(question)\n",
    "print(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 뉴스 기반 챗봇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://n.news.naver.com/article/437/0000378416'}, page_content=\"\\n출산 직원에게 '1억원' 쏜다…회사의 파격적 저출생 정책\\n\\n\\n[앵커]올해 아이 낳을 계획이 있는 가족이라면 솔깃할 소식입니다. 정부가 저출생 대책으로 매달 주는 부모 급여, 0세 아이는 100만원으로 올렸습니다. 여기에 첫만남이용권, 아동수당까지 더하면 아이 돌까지 1년 동안 1520만원을 받습니다. 지자체도 경쟁하듯 지원에 나섰습니다. 인천시는 새로 태어난 아기, 18살될 때까지 1억원을 주겠다. 광주시도 17살될 때까지 7400만원 주겠다고 했습니다. 선거 때면 나타나서 아이 낳으면 현금 주겠다고 밝힌 사람이 있었죠. 과거에는 표만 노린 '황당 공약'이라는 비판이 따라다녔습니다. 그런데 지금은 출산율이 이보다 더 나쁠 수 없다보니, 이런 현금성 지원을 진지하게 정책화 하는 상황까지 온 겁니다. 게다가 기업들도 뛰어들고 있습니다. 이번에는 출산한 직원에게 단번에 1억원을 주겠다는 회사까지 나타났습니다.이상화 기자가 취재했습니다.[기자]한 그룹사가 오늘 파격적인 저출생 정책을 내놨습니다.2021년 이후 태어난 직원 자녀에 1억원씩, 총 70억원을 지원하고 앞으로도 이 정책을 이어가기로 했습니다.해당 기간에 연년생과 쌍둥이 자녀가 있으면 총 2억원을 받게 됩니다.[오현석/부영그룹 직원 : 아이 키우는 데 금전적으로 많이 힘든 세상이잖아요. 교육이나 생활하는 데 큰 도움이 될 거라 생각합니다.]만약 셋째까지 낳는 경우엔 국민주택을 제공하겠다는 뜻도 밝혔습니다.[이중근/부영그룹 회장 : 3년 이내에 세 아이를 갖는 분이 나올 것이고 따라서 주택을 제공할 수 있는 계기가 될 것으로 생각하고.][조용현/부영그룹 직원 : 와이프가 셋째도 갖고 싶어 했는데 경제적 부담 때문에 부정적이었거든요. (이제) 긍정적으로 생각할 수 있을 것 같습니다.]오늘 행사에서는, 회사가 제공하는 출산장려금은 받는 직원들의 세금 부담을 고려해 정부가 면세해달라는 제안도 나왔습니다.이같은 출산장려책은 점점 확산하는 분위기입니다.법정기간보다 육아휴직을 길게 주거나, 남성 직원의 육아휴직을 의무화한 곳도 있습니다.사내 어린이집을 밤 10시까지 운영하고 셋째를 낳으면 무조건 승진시켜 주기도 합니다.한 회사는 지난해 네쌍둥이를 낳은 직원에 의료비를 지원해 관심을 모았습니다.정부 대신 회사가 나서는 출산장려책이 사회적 분위기를 바꿀 거라는 기대가 커지는 가운데, 여력이 부족한 중소지원이 필요하다는 목소리도 나옵니다.[영상디자인 곽세미]\\n\\t\\t\\n\")]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_path=(\"https://n.news.naver.com/article/437/0000378416\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            \"div\",\n",
    "            attrs={\"class\": [\"newsct_article _article_body\", \"media_end_head_title\"]},\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(f'문서의 수: {len(docs)}')\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,\n",
    "                                                chunk_overlap = 100)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(documents=splits, embedding = OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"당신은 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다. 당신의 임무는 주어진 문맥(context) 에서 주어진 질문(question) 에 답하는 것입니다.\n",
    "검색된 다음 문맥(context) 을 사용하여 질문(question) 에 답하세요. 만약, 주어진 문맥(context) 에서 답을 찾을 수 없다면, 답을 모른다면 `주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다` 라고 답하세요.\n",
    "한글로 답변해 주세요. 단, 기술적인 용어나 이름은 번역하지 않고 그대로 사용해 주세요. Don't narrate the answer, just answer the question. Let's think step-by-step.\n",
    "\n",
    "#Question: \n",
    "{question} \n",
    "\n",
    "#Context: \n",
    "{context} \n",
    "\n",
    "#Answer:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "# 체인을 생성합니다.\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamChain:\n",
    "    def __init__(self, chain):\n",
    "        self.chain = chain\n",
    "\n",
    "    def stream(self, query):\n",
    "        response = self.chain.stream(query)\n",
    "        complete_response = \"\"\n",
    "        for token in response:\n",
    "            print(token, end=\"\", flush=True)\n",
    "            complete_response += token\n",
    "        return complete_response\n",
    "\n",
    "\n",
    "# 생성자에 chain 을 매개변수로 전달하여 chain 객체를 생성합니다.\n",
    "chain = StreamChain(rag_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부영그룹은 출산한 직원에게 1억원을 지원합니다."
     ]
    }
   ],
   "source": [
    "answer = chain.stream(\"부영그룹은 출산 직원에게 얼마의 지원을 제공하나요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 부모 급여: 0세 아이에게 매달 100만원 지급\n",
      "- 첫만남이용권 제공\n",
      "- 아동수당 지급\n",
      "- 지자체별 지원금: 인천시는 18살까지 1억원, 광주시는 17살까지 7400만원 지급\n",
      "- 기업의 출산장려금: 출산한 직원에게 1억원 지급\n",
      "- 셋째 아이 출산 시 국민주택 제공\n",
      "- 육아휴직 연장 및 남성 직원의 육아휴직 의무화\n",
      "- 사내 어린이집 운영 시간 연장 (밤 10시까지)"
     ]
    }
   ],
   "source": [
    "answer = chain.stream(\"정부의 저출생 대책을 bullet points 형식으로 작성해 주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다."
     ]
    }
   ],
   "source": [
    "answer = chain.stream(\"부영그룹의 임직원 숫자는 몇명인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://n.news.naver.com/article/437/0000378416\n",
      "문서의 수: 1\n",
      "============================================================\n",
      "[HUMAN]\n",
      "부영그룹의 출산 장려 정책에 대해 설명해주세요\n",
      "\n",
      "[AI]\n",
      "부영그룹은 출산을 장려하기 위해 2021년 이후 태어난 직원 자녀에 1억원씩, 총 70억원을 지원하고 앞으로도 이 정책을 이어가기로 했습니다. 해당 기간에 연년생과 쌍둥이 자녀가 있으면 총 2억원을 받게 됩니다. 또한, 셋째까지 낳는 경우에는 국민주택을 제공할 예정입니다.\n"
     ]
    }
   ],
   "source": [
    "# 단계 1: 문서 로드(Load Documents)\n",
    "# 뉴스기사 내용을 로드하고, 청크로 나누고, 인덱싱합니다.\n",
    "url = \"https://n.news.naver.com/article/437/0000378416\"\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(url,),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            \"div\",\n",
    "            attrs={\"class\": [\"newsct_article _article_body\", \"media_end_head_title\"]},\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "# 단계 2: 문서 분할(Split Documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 단계 3: 임베딩 & 벡터스토어 생성(Create Vectorstore)\n",
    "# 벡터스토어를 생성합니다.\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# 단계 4: 검색(Search)\n",
    "# 뉴스에 포함되어 있는 정보를 검색하고 생성합니다.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 단계 5: 프롬프트 생성(Create Prompt)\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# 단계 6: 언어모델 생성(Create LLM)\n",
    "# 모델(LLM) 을 생성합니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    # 검색한 문서 결과를 하나의 문단으로 합쳐줍니다.\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# 단계 7: 체인 생성(Create Chain)\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 단계 8: 체인 실행(Run Chain)\n",
    "# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\n",
    "question = \"부영그룹의 출산 장려 정책에 대해 설명해주세요\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"URL: {url}\")\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "print(\"===\" * 20)\n",
    "print(f\"[HUMAN]\\n{question}\\n\")\n",
    "print(f\"[AI]\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Could AI \\'trading bots\\' transform the world of investing?Getty ImagesIt is hard for both humans and computers to predict stock market movementsSearch for \"AI investing\" online, and you\\'ll be flooded with endless offers to let artificial intelligence manage your money.I recently spent half an hour finding out what so-called AI \"trading bots\" could apparently do with my investments.Many prominently suggest that they can give me lucrative returns. Yet as every reputable financial firm warns - your '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 뉴스기사의 내용을 로드하고, 청크로 나누고, 인덱싱합니다.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://www.bbc.com/news/business-68092814\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            \"main\",\n",
    "            attrs={\"id\": [\"main-content\"]},\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "docs[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 23\n",
      "\n",
      "[페이지내용]\n",
      "SPRi AI Brief |  \n",
      "2023-12 월호\n",
      "8코히어 , 데이터 투명성 확보를 위한 데이터 출처 탐색기 공개\n",
      "n코히어와 12개 기관이  광범위한 데이터셋에 대한 감사를 통해 원본 데이터 출처, 재라이선스 상태, \n",
      "작성자 등 다양한 정보를 제공하는 ‘데이터 출처 탐색기 ’ 플랫폼을 출시\n",
      "n대화형 플랫폼을 통해 개발자는 데이터셋의 라이선스 상태를 쉽게 파악할 수 있으며 데이터셋의 \n",
      "구성과 계보도 추적 가능KEY Contents\n",
      "£데이터 출처 탐색기 , 광범위한 데이터셋 정보 제공을 통해 데이터 투명성 향상\n",
      "nAI 기업 코히어 (Cohere) 가 매사추세츠 공과⼤(MIT), 하버드 ⼤ 로스쿨 , 카네기멜론 ⼤ 등 12개 기관과  \n",
      "함께 2023 년 10월 25일 ‘데이터 출처 탐색기 (Data Provenance Explorer)’ 플랫폼을 공개\n",
      "∙AI 모델 훈련에 사용되는 데이터셋의 불분명한 출처로 인해 데이터 투명성이 확보되지 않아 다양한 \n",
      "법적·윤리적 문제가 발생\n",
      "∙이에 연구\n",
      "\n",
      "[metadata]\n",
      "{'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 10}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일 로드. 파일의 경로 입력\n",
    "loader = PyPDFLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "\n",
    "# 페이지 별 문서 로드\n",
    "docs = loader.load()\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "\n",
    "# 10번째 페이지의 내용 출력\n",
    "print(f\"\\n[페이지내용]\\n{docs[10].page_content[:500]}\")\n",
    "print(f\"\\n[metadata]\\n{docs[10].metadata}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 891\n",
      "\n",
      "[페이지내용]\n",
      "PassengerId: 11\n",
      "Survived: 1\n",
      "Pclass: 3\n",
      "Name: Sandstrom, Miss. Marguerite Rut\n",
      "Sex: female\n",
      "Age: 4\n",
      "SibSp: 1\n",
      "Parch: 1\n",
      "Ticket: PP 9549\n",
      "Fare: 16.7\n",
      "Cabin: G6\n",
      "Embarked: S\n",
      "\n",
      "[metadata]\n",
      "{'source': 'data/titanic.csv', 'row': 10}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# CSV 파일 로드\n",
    "loader = CSVLoader(file_path=\"data/titanic.csv\")\n",
    "docs = loader.load()\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "\n",
    "# 10번째 페이지의 내용 출력\n",
    "print(f\"\\n[페이지내용]\\n{docs[10].page_content[:500]}\")\n",
    "print(f\"\\n[metadata]\\n{docs[10].metadata}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 1\n",
      "\n",
      "[페이지내용]\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
      "연관키워드: 토큰화, 자연어\n",
      "\n",
      "[metadata]\n",
      "{'source': 'data/appendix-keywords.txt'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/appendix-keywords.txt\")\n",
    "docs = loader.load()\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "\n",
    "# 10번째 페이지의 내용 출력\n",
    "print(f\"\\n[페이지내용]\\n{docs[0].page_content[:500]}\")\n",
    "print(f\"\\n[metadata]\\n{docs[0].metadata}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s][nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\UserK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 1\n",
      "\n",
      "[페이지내용]\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다. 예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어\n",
      "\n",
      "[metadata]\n",
      "{'source': 'data\\\\appendix-keywords.txt'}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\".\", glob=\"data/*.txt\", show_progress=True)\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "\n",
    "# 10번째 페이지의 내용 출력\n",
    "print(f\"\\n[페이지내용]\\n{docs[0].page_content[:500]}\")\n",
    "print(f\"\\n[metadata]\\n{docs[0].metadata}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 1\n",
      "\n",
      "[메타데이터]\n",
      "\n",
      "{'source': 'data\\\\SPRI_AI_Brief_2023년12월호_F.pdf'}\n",
      "\n",
      "========= [앞부분] 미리보기 =========\n",
      "\n",
      "템을 개발하는 기업에게 안전 테스트 결과와 시스템에 관한\n",
      "\n",
      "주요 정보를 미국 정부와 공유할 것을 요구하고, AI 시스템의 안전성과 신뢰성 확인을 위한 표준 및\n",
      "\n",
      "AI 생성 콘텐츠 표시를 위한 표준과 모범사례 확립을 추진\n",
      "\n",
      "△1026 플롭스(FLOPS, Floating Point Operation Per Second)를 초과하는 컴퓨팅 성능 또는 생물학적 서열 데이터를 주로 사용하고 1023플롭스를 초과하는 컴퓨팅 성능을 사용하는 모델 △단일 데이터센터에서 1,000Gbit/s 이상의 네트워킹으로 연결되며 AI 훈련에서 이론상 최대 1020 플롭스를 처리할 수 있는 컴퓨팅 용량을 갖춘 컴퓨팅 클러스터가 정보공유 요구대상\n",
      "\n",
      "n (형평성과 시민권 향상) 법률, 주택, 보건 분야에서 AI의 무책임한 사용으로 인한 차별과 편견 및 기타\n",
      "\n",
      "문제를 방지하는 조치를 확대\n",
      "\n",
      "형사사법 시스템에서 AI 사용 모범사례를 개발하고, 주택 임대 시 AI 알고리즘 차별을 막기 위한 명확한\n",
      "\n",
      "지침을 제공하며, \n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\".\", glob=\"data/*.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"문서의 수: {len(docs)}\\n\")\n",
    "print(\"[메타데이터]\\n\")\n",
    "print(docs[0].metadata)\n",
    "print(\"\\n========= [앞부분] 미리보기 =========\\n\")\n",
    "print(docs[0].page_content[2500:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 1\n",
      "\n",
      "[메타데이터]\n",
      "\n",
      "{'source': 'data\\\\audio_utils.py'}\n",
      "\n",
      "========= [앞부분] 미리보기 =========\n",
      "\n",
      "import re\n",
      "import os\n",
      "from pytube import YouTube\n",
      "from moviepy.editor import AudioFileClip, VideoFileClip\n",
      "from pydub import AudioSegment\n",
      "from pydub.silence import detect_nonsilent\n",
      "\n",
      "\n",
      "def extract_abr(abr):\n",
      "    youtube_audio_pattern = re.compile(r\"\\d+\")\n",
      "    kbps = youtube_audio_pattern.search(abr)\n",
      "    if kbps:\n",
      "        kbps = kbps.group()\n",
      "        return int(kbps)\n",
      "    else:\n",
      "        return 0\n",
      "\n",
      "\n",
      "def get_audio_filepath(filename):\n",
      "    # audio 폴더가 없으면 생성\n",
      "    if not os.path.isdir(\"audio\"):\n",
      "        os.mkdir(\"au\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PythonLoader\n",
    "\n",
    "loader = DirectoryLoader(\".\", glob=\"**/*.py\", loader_cls=PythonLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"문서의 수: {len(docs)}\\n\")\n",
    "print(\"[메타데이터]\\n\")\n",
    "print(docs[0].metadata)\n",
    "print(\"\\n========= [앞부분] 미리보기 =========\\n\")\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Could AI \\'trading bots\\' transform the world of investing?Getty ImagesIt is hard for both humans and computers to predict stock market movementsSearch for \"AI investing\" online, and you\\'ll be flooded with endless offers to let artificial intelligence manage your money.I recently spent half an hour finding out what so-called AI \"trading bots\" could apparently do with my investments.Many prominently suggest that they can give me lucrative returns. Yet as every reputable financial firm warns - your '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 뉴스기사의 내용을 로드하고, 청크로 나누고, 인덱싱합니다.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://www.bbc.com/news/business-68092814\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            \"main\",\n",
    "            attrs={\"id\": [\"main-content\"]},\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "docs[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator= '\\n\\n',\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 10,\n",
    "    length_function = len,\n",
    "    is_separator_regex=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain of density 논문의 일부 내용을 불러옵니다\n",
    "with open(\"data/chain-of-density.txt\", \"r\") as f:\n",
    "    text = f.read()[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Selecting the “right” amount of information to include in a summary is a difficult task. \\nA good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries genera']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Selecting the “right” amount of information to include in a summary is a difficult task.',\n",
       " 'A good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries genera']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10, separator=\"\\n\")\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Selecting the “right” amount of information to include in a summary is a difficult task. \\nA good',\n",
       " 'A good summary should be detailed and entity-centric without being overly dense and hard to follow.',\n",
       " 'to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with',\n",
       " 'with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial',\n",
       " 'an initial entity-sparse summary before iteratively incorporating missing salient entities without',\n",
       " 'without increasing the length. Summaries genera']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10, separator=\" \")\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Selecting the “right” amount of information to include in a summary is a difficult task. \\nA good',\n",
       " 'summary should be detailed and entity-centric without being overly dense and hard to follow. To',\n",
       " 'better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to',\n",
       " 'as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary',\n",
       " 'before iteratively incorporating missing salient entities without increasing the length. Summaries',\n",
       " 'genera']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0, separator=\" \")\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://www.bbc.com/news/business-68092814'}, page_content='Could AI \\'trading bots\\' transform the world of investing?Getty ImagesIt is hard for both humans and computers to predict stock market movementsSearch for \"AI investing\" online, and you\\'ll be flooded with endless offers to let artificial intelligence manage your money.I recently spent half an hour finding out what so-called AI \"trading bots\" could apparently do with my investments.Many prominently suggest that they can give me lucrative returns. Yet as every reputable financial firm warns - your capital may be at risk.Or putting it more simply - you could lose your money - whether it is a human or a computer that is making stock market decisions on your behalf.Yet such has been the hype about the ability of AI over the past few years, that almost one in three investors would be happy to let a trading bot make all the decisions for them, according to one 2023 survey in the US.John Allan says investors should be more cautious about using AI. He is head of innovation and operations for the')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100, separator=\" \")\n",
    "# text 파일을 청크로 나누어줍니다.\n",
    "text_splitter.split_text(text)\n",
    "\n",
    "# document를 청크로 나누어줍니다.\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "print(len(split_docs))\n",
    "split_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain 패키지에서 RecursiveCharacterTextSplitter 클래스를 가져옵니다.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # 정말 작은 청크 크기를 설정합니다.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain of density 논문의 일부 내용을 불러옵니다\n",
    "with open(\"data/chain-of-density.txt\", \"r\") as f:\n",
    "    text = f.read()[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting the “right” amount of information to include in a summary is a difficult task. \n",
      "A good\n",
      "A good summary should be detailed and entity-centric without being overly dense and hard to follow.\n",
      "to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with\n",
      "with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial\n",
      "an initial entity-sparse summary before iteratively incorporating missing salient entities without\n",
      "without increasing the length. Summaries genera\n",
      "============================================================\n",
      "Selecting the “right” amount of information to include in a summary is a difficult task.\n",
      "A good summary should be detailed and entity-centric without being overly dense and hard to follow.\n",
      "follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what\n",
      "with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an\n",
      "an initial entity-sparse summary before iteratively incorporating missing salient entities without\n",
      "without increasing the length. Summaries genera\n"
     ]
    }
   ],
   "source": [
    "character_text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=10, separator=\" \"\n",
    ")\n",
    "for sent in character_text_splitter.split_text(text):\n",
    "    print(sent)\n",
    "print(\"===\" * 20)\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=10\n",
    ")\n",
    "for sent in recursive_text_splitter.split_text(text):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n', '\\n', ' ', '']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recursive_text_splitter 에 기본 지정된 separators 를 확인합니다.\n",
    "recursive_text_splitter._separators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'data\\\\Statistics 11th Edition_240709_201017.pdf', 'file_path': 'data\\\\Statistics 11th Edition_240709_201017.pdf', 'page': 3, 'total_pages': 501, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Samsung Electronics', 'producer': 'Samsung Electronics', 'creationDate': 'D:', 'modDate': \"D:20240710055403+09'00'\", 'trapped': ''}, page_content='the environmental, social, economic, and ethical challenges we face in our business. Among the issues we are \\naddressing are carbon impact, paper specifications and procurement, ethical conduct within our business and \\namong our vendors, and community and charitable support. For more information, please visit our website: \\nwww.wiley.com/go/citizenship.\\nCopyright © 2017, 2010, 2007 John Wiley & Sons, Inc. All rights reserved. \\nNo part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by \\nany means, electronic, mechanical, photocopying, recording, scanning or otherwise, except as permitted un-\\nder Sections 107 or 108 of the 1976 United States Copyright Act, without either the prior written permission \\nof the Publisher, or authorization through payment of the appropriate per-copy fee to the Copyright Clear-\\nance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923 (Web site: www.copyright.com). Requests to'), Document(metadata={'source': 'data\\\\Statistics 11th Edition_240709_201017.pdf', 'file_path': 'data\\\\Statistics 11th Edition_240709_201017.pdf', 'page': 3, 'total_pages': 501, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Samsung Electronics', 'producer': 'Samsung Electronics', 'creationDate': 'D:', 'modDate': \"D:20240710055403+09'00'\", 'trapped': ''}, page_content='VP AND EDITORIAL DIRECTOR  \\nGeorge Hoffman\\nEDITORIAL DIRECTOR \\nVeronica Visentin\\nEDITORIAL ASSISTANT \\nEthan Lipson\\nEDITORIAL MANAGER   \\nGladys Soto\\nCONTENT MANAGEMENT DIRECTOR \\nLisa Wojcik\\nCONTENT MANAGER \\nNichole Urban\\nSENIOR CONTENT SPECIALIST \\nNicole Repasky\\nPRODUCTION EDITOR \\nAbidha Sulaiman\\nCOVER PHOTO CREDIT \\nM.C. Escher’s Spirals © The M.C. Escher Company \\n- The Netherlands\\nThis book was set in 10/11 Times LT Std by SPi Global and printed and bound by Lightning Source Inc. The \\ncover was printed by Lightning Source Inc.\\nFounded in 1807, John Wiley & Sons, Inc. has been a valued source of knowledge and understanding for \\nmore than 200 years, helping people around the world meet their needs and fulfill their aspirations. Our \\ncompany is built on a foundation of principles that include responsibility to the communities we serve and \\nwhere we live and work. In 2008, we launched a Corporate Citizenship Initiative, a global effort to address'), Document(metadata={'source': 'data\\\\Statistics 11th Edition_240709_201017.pdf', 'file_path': 'data\\\\Statistics 11th Edition_240709_201017.pdf', 'page': 60, 'total_pages': 501, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Samsung Electronics', 'producer': 'Samsung Electronics', 'creationDate': 'D:', 'modDate': \"D:20240710055403+09'00'\", 'trapped': ''}, page_content='(b) What is the shape of this distribution?'), Document(metadata={'source': 'data\\\\Statistics 11th Edition_240709_201017.pdf', 'file_path': 'data\\\\Statistics 11th Edition_240709_201017.pdf', 'page': 142, 'total_pages': 501, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Samsung Electronics', 'producer': 'Samsung Electronics', 'creationDate': 'D:', 'modDate': \"D:20240710055403+09'00'\", 'trapped': ''}, page_content='6.12 An extensive correlation study indicates that a longer life is experienced by people \\nwho follow the seven “golden rules” of behavior, including moderate drinking, no \\nsmoking, regular meals, some exercise, and eight hours of sleep each night. Can we \\nconclude, therefore, that this type of behavior causes a longer life?')]\n"
     ]
    }
   ],
   "source": [
    "query = \"회사의 저출생 정책이 뭐야?\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\")\n",
    "search_result = retriever.get_relevant_documents(query)\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.8\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "query = \"회사의 저출생 정책이 뭐야?\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.8}\n",
    ")\n",
    "search_result = retriever.get_relevant_documents(query)\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'data\\\\Statistics 11th Edition_240709_201017.pdf', 'file_path': 'data\\\\Statistics 11th Edition_240709_201017.pdf', 'page': 3, 'total_pages': 501, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Samsung Electronics', 'producer': 'Samsung Electronics', 'creationDate': 'D:', 'modDate': \"D:20240710055403+09'00'\", 'trapped': ''}, page_content='the environmental, social, economic, and ethical challenges we face in our business. Among the issues we are \\naddressing are carbon impact, paper specifications and procurement, ethical conduct within our business and \\namong our vendors, and community and charitable support. For more information, please visit our website: \\nwww.wiley.com/go/citizenship.\\nCopyright © 2017, 2010, 2007 John Wiley & Sons, Inc. All rights reserved. \\nNo part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by \\nany means, electronic, mechanical, photocopying, recording, scanning or otherwise, except as permitted un-\\nder Sections 107 or 108 of the 1976 United States Copyright Act, without either the prior written permission \\nof the Publisher, or authorization through payment of the appropriate per-copy fee to the Copyright Clear-\\nance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923 (Web site: www.copyright.com). Requests to'), Document(metadata={'source': 'data\\\\Statistics 11th Edition_240709_201017.pdf', 'file_path': 'data\\\\Statistics 11th Edition_240709_201017.pdf', 'page': 60, 'total_pages': 501, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Samsung Electronics', 'producer': 'Samsung Electronics', 'creationDate': 'D:', 'modDate': \"D:20240710055403+09'00'\", 'trapped': ''}, page_content='(b) What is the shape of this distribution?')]\n"
     ]
    }
   ],
   "source": [
    "query = \"회사의 저출생 정책이 뭐야?\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2})\n",
    "search_result = retriever.get_relevant_documents(query)\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "query = \"회사의 저출생 정책이 뭐야?\"\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What is the definition of anova?', 'Can you explain the concept of anova?', 'How would you describe anova in simple terms?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = [\n",
    "    \"난 오늘 많이 먹어서 배가 정말 부르다\",\n",
    "    \"떠나는 저 배가 오늘 마지막 배인가요?\",\n",
    "    \"내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\",\n",
    "]\n",
    "\n",
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_texts(doc_list)\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "faiss_vectorstore = FAISS.from_texts(doc_list, OpenAIEmbeddings())\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(docs):\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"[{i+1}] {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query]\n",
      "나 요즘 배에 정말 살이 많이 쪘어...\n",
      "\n",
      "[BM25 Retriever]\n",
      "[1] 난 오늘 많이 먹어서 배가 정말 부르다\n",
      "[2] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\n",
      "============================================================\n",
      "[FAISS Retriever]\n",
      "[1] 난 오늘 많이 먹어서 배가 정말 부르다\n",
      "[2] 떠나는 저 배가 오늘 마지막 배인가요?\n",
      "============================================================\n",
      "[Ensemble Retriever]\n",
      "[1] 난 오늘 많이 먹어서 배가 정말 부르다\n",
      "[2] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\n",
      "[3] 떠나는 저 배가 오늘 마지막 배인가요?\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"나 요즘 배에 정말 살이 많이 쪘어...\"\n",
    "print(f\"[Query]\\n{sample_query}\\n\")\n",
    "relevant_docs = bm25_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[BM25 Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = faiss_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[FAISS Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = ensemble_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[Ensemble Retriever]\")\n",
    "pretty_print(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query]\n",
      "바다 위에 떠다니는 배들이 많다\n",
      "\n",
      "[BM25 Retriever]\n",
      "[1] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\n",
      "[2] 떠나는 저 배가 오늘 마지막 배인가요?\n",
      "============================================================\n",
      "[FAISS Retriever]\n",
      "[1] 난 오늘 많이 먹어서 배가 정말 부르다\n",
      "[2] 떠나는 저 배가 오늘 마지막 배인가요?\n",
      "============================================================\n",
      "[Ensemble Retriever]\n",
      "[1] 떠나는 저 배가 오늘 마지막 배인가요?\n",
      "[2] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\n",
      "[3] 난 오늘 많이 먹어서 배가 정말 부르다\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"바다 위에 떠다니는 배들이 많다\"\n",
    "print(f\"[Query]\\n{sample_query}\\n\")\n",
    "relevant_docs = bm25_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[BM25 Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = faiss_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[FAISS Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = ensemble_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[Ensemble Retriever]\")\n",
    "pretty_print(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query]\n",
      "ships\n",
      "\n",
      "[BM25 Retriever]\n",
      "[1] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\n",
      "[2] 떠나는 저 배가 오늘 마지막 배인가요?\n",
      "============================================================\n",
      "[FAISS Retriever]\n",
      "[1] 떠나는 저 배가 오늘 마지막 배인가요?\n",
      "[2] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\n",
      "============================================================\n",
      "[Ensemble Retriever]\n",
      "[1] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\n",
      "[2] 떠나는 저 배가 오늘 마지막 배인가요?\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"ships\"\n",
    "print(f\"[Query]\\n{sample_query}\\n\")\n",
    "relevant_docs = bm25_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[BM25 Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = faiss_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[FAISS Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = ensemble_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[Ensemble Retriever]\")\n",
    "pretty_print(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query]\n",
      "pear\n",
      "\n",
      "[BM25 Retriever]\n",
      "[1] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\n",
      "[2] 떠나는 저 배가 오늘 마지막 배인가요?\n",
      "============================================================\n",
      "[FAISS Retriever]\n",
      "[1] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\n",
      "[2] 떠나는 저 배가 오늘 마지막 배인가요?\n",
      "============================================================\n",
      "[Ensemble Retriever]\n",
      "[1] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\n",
      "[2] 떠나는 저 배가 오늘 마지막 배인가요?\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"pear\"\n",
    "print(f\"[Query]\\n{sample_query}\\n\")\n",
    "relevant_docs = bm25_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[BM25 Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = faiss_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[FAISS Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = ensemble_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[Ensemble Retriever]\")\n",
    "pretty_print(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 39\n",
      "\tPrompt Tokens: 24\n",
      "\tCompletion Tokens: 15\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $3.45e-05\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = model.invoke(\"대한민국의 수도는 어디인가요?\")\n",
    "print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement HuggingFaceHub (from versions: none)\n",
      "ERROR: No matching distribution found for HuggingFaceHub\n"
     ]
    }
   ],
   "source": [
    "!pip install HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFaceHub 객체 생성\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "# Hugging Face API 토큰 설정\n",
    "api_token = \"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\"\n",
    "\n",
    "repo_id = \"google/flan-t5-xxl\"\n",
    "\n",
    "t5_model = HuggingFaceHub(\n",
    "    repo_id=repo_id, \n",
    "    huggingfacehub_api_token=api_token,\n",
    "    model_kwargs={\"temperature\": 0.1, \"max_length\": 512}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": " (Request ID: YljAMk9-cmSC4NvWyKGj1)\n\nBad request:\nAuthorization header is correct, but the token seems invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/google/flan-t5-xxl",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 15\u001b[0m\n\u001b[0;32m      8\u001b[0m t5_model \u001b[38;5;241m=\u001b[39m HuggingFaceHub(\n\u001b[0;32m      9\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39mrepo_id, \n\u001b[0;32m     10\u001b[0m     huggingfacehub_api_token\u001b[38;5;241m=\u001b[39mapi_token,\n\u001b[0;32m     11\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m512\u001b[39m}\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 모델 호출\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mt5_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhere is the capital of South Korea?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     emit_warning()\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1192\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1190\u001b[0m     )\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 1192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1200\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1201\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m   1202\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:882\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    868\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    869\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    870\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    880\u001b[0m         )\n\u001b[0;32m    881\u001b[0m     ]\n\u001b[1;32m--> 882\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:740\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    739\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    741\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:727\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    718\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    719\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    724\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    725\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    726\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 727\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    731\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    735\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    736\u001b[0m         )\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    738\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1431\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1430\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1431\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1432\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1433\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1434\u001b[0m     )\n\u001b[0;32m   1435\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\langchain_community\\llms\\huggingface_hub.py:139\u001b[0m, in \u001b[0;36mHuggingFaceHub._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m _model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    137\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_model_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m--> 139\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:273\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 273\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:358\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[0;32m    355\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m endpoint:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoint_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    357\u001b[0m     )\n\u001b[1;32m--> 358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[0;32m    361\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf you are trying to create or update content,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake sure you have a token with the `write` role.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n",
      "\u001b[1;31mBadRequestError\u001b[0m:  (Request ID: YljAMk9-cmSC4NvWyKGj1)\n\nBad request:\nAuthorization header is correct, but the token seems invalid"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "# Hugging Face API 토큰 설정\n",
    "api_token = \"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\"\n",
    "\n",
    "repo_id = \"google/flan-t5-xxl\"\n",
    "\n",
    "t5_model = HuggingFaceHub(\n",
    "    repo_id=repo_id, \n",
    "    huggingfacehub_api_token=api_token,\n",
    "    model_kwargs={\"temperature\": 0.1, \"max_length\": 512}\n",
    ")\n",
    "\n",
    "# 모델 호출\n",
    "response = t5_model(\"Where is the capital of South Korea?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Path: data/SPRI_AI_Brief_2023년12월호_F.pdf\n",
      "문서의 수: 1\n",
      "============================================================\n",
      "[HUMAN]\n",
      "anova가 뭐야?\n",
      "\n",
      "[AI]\n",
      "ANOVA(분산분석)은 두 개 이상의 모집단 평균에 대한 귀무가설을 검정하는 전체적인 테스트입니다. 가장 간단한 형태의 ANOVA는 한 가지 독립 변수에 의해 분류된 모집단 평균 간의 차이를 테스트하는 일원분산분석(One-Factor ANOVA)입니다.\n"
     ]
    }
   ],
   "source": [
    "# 단계 1: 문서 로드(Load Documents)\n",
    "# 문서를 로드하고, 청크로 나누고, 인덱싱합니다.\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일 로드. 파일의 경로 입력\n",
    "file_path = \"data/SPRI_AI_Brief_2023년12월호_F.pdf\"\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "\n",
    "# 단계 2: 문서 분할(Split Documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "split_docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "\n",
    "# 단계 3, 4: 임베딩 & 벡터스토어 생성(Create Vectorstore)\n",
    "# 벡터스토어를 생성합니다.\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# 단계 5: 리트리버 생성(Create Retriever)\n",
    "# 사용자의 질문(query) 에 부합하는 문서를 검색합니다.\n",
    "\n",
    "# 유사도 높은 K 개의 문서를 검색합니다.\n",
    "k = 3\n",
    "\n",
    "# (Sparse) bm25 retriever and (Dense) faiss retriever 를 초기화 합니다.\n",
    "bm25_retriever = BM25Retriever.from_documents(split_docs)\n",
    "bm25_retriever.k = k\n",
    "\n",
    "faiss_vectorstore = FAISS.from_documents(split_docs, OpenAIEmbeddings())\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "# 단계 6: 프롬프트 생성(Create Prompt)\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# 단계 7: 언어모델 생성(Create LLM)\n",
    "# 모델(LLM) 을 생성합니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    # 검색한 문서 결과를 하나의 문단으로 합쳐줍니다.\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# 단계 8: 체인 생성(Create Chain)\n",
    "rag_chain = (\n",
    "    {\"context\": ensemble_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"PDF Path: {file_path}\")\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "print(\"===\" * 20)\n",
    "print(f\"[HUMAN]\\n{question}\\n\")\n",
    "print(f\"[AI]\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼성전자가 자체 개발한 생성 AI 모델 '삼성 가우스'는 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성되어 있습니다. 삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없는 안전한 데이터를 통해 학습되었고, 다양한 상황에 최적화된 크기의 모델 선택이 가능합니다. 이 모델은 텍스트, 코드, 이미지를 생성하는 세 가지 모델로 구성되어 있으며, 다양한 업무 처리 및 소프트웨어 개발에 활용될 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 단계 8: 체인 실행(Run Chain)\n",
    "# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\n",
    "question = \"삼성 가우스에 대해 설명해주세요\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2027년 AI 소프트웨어 매출은 2,510 억 달러로 예상되며, AI 애플리케이션은 2027년까지 21.1%의 연평균 성장률을 기록할 것으로 전망됩니다. AI 소프트웨어 시장은 AI 플랫폼, AI 애플리케이션, AI 시스템 인프라 소프트웨어를 포괄하고 있습니다.생성 AI 플랫폼과 애플리케이션은 2027년까지 283억 달러의 매출을 창출할 것으로 전망됩니다.\n"
     ]
    }
   ],
   "source": [
    "# 단계 8: 체인 실행(Run Chain)\n",
    "# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\n",
    "question = \"미래의 AI 소프트웨어 매출 전망은 어떻게 되나요?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유튜브는 2024년부터 AI 생성 콘텐츠에 AI 라벨 표시를 의무화하고, 이를 준수하지 않는 콘텐츠는 삭제하고 크리에이터에 대한 수익 배분도 중단할 수 있다고 발표했습니다. 이러한 규칙은 AI로 제작된 콘텐츠가 신원 파악이 가능한 개인을 모방한 경우에도 적용됩니다. 유튜브는 AI를 이용한 콘텐츠의 변경이나 합성 여부를 시청자에게 전달하기 위해 라벨을 표시하는 방식을 도입할 예정입니다.\n"
     ]
    }
   ],
   "source": [
    "# 단계 8: 체인 실행(Run Chain)\n",
    "# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\n",
    "question = \"YouTube 가 2024년에 의무화 한 것은 무엇인가요?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# 프롬프트 정의\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"당신은 Question-Answering 챗봇입니다. 주어진 질문에 대한 답변을 제공해주세요.\",\n",
    "        ),\n",
    "        # 대화기록용 key 인 chat_history 는 가급적 변경 없이 사용하세요!\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"#Question:\\n{question}\"),  # 사용자 입력을 변수로 사용\n",
    "    ]\n",
    ")\n",
    "\n",
    "# llm 생성\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# 일반 Chain 생성\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세션 기록을 저장할 딕셔너리\n",
    "store = {}\n",
    "\n",
    "\n",
    "# 세션 ID를 기반으로 세션 기록을 가져오는 함수\n",
    "def get_session_history(session_ids):\n",
    "    print(f\"[대화 세션ID]: {session_ids}\")\n",
    "    if session_ids not in store:  # 세션 ID가 store에 없는 경우\n",
    "        # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\n",
    "        store[session_ids] = ChatMessageHistory()\n",
    "    return store[session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\n",
    "\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,  # 세션 기록을 가져오는 함수\n",
    "    input_messages_key=\"question\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\n",
    "    history_messages_key=\"chat_history\",  # 기록 메시지의 키\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션ID]: abc123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'안녕하세요, 테디님! 무엇을 도와드릴까요?'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke(\n",
    "    # 질문 입력\n",
    "    {\"question\": \"나의 이름은 테디입니다.\"},\n",
    "    # 세션 ID 기준으로 대화를 기록합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션ID]: abc123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"당신의 이름은 '테디'입니다.\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke(\n",
    "    # 질문 입력\n",
    "    {\"question\": \"내 이름이 뭐라고?\"},\n",
    "    # 세션 ID 기준으로 대화를 기록합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "# 단계 1: 문서 로드(Load Documents)\n",
    "loader = PDFPlumberLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 단계 2: 문서 분할(Split Documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# 단계 3: 임베딩(Embedding) 생성\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 단계 4: DB 생성(Create DB) 및 저장\n",
    "# 벡터스토어를 생성합니다.\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "\n",
    "# 단계 5: 검색기(Retriever) 생성\n",
    "# 문서에 포함되어 있는 정보를 검색하고 생성합니다.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 단계 6: 프롬프트 생성(Create Prompt)\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean.\n",
    "\n",
    "#Previous Chat History:\n",
    "{chat_history}\n",
    "\n",
    "#Question: \n",
    "{question} \n",
    "\n",
    "#Context: \n",
    "{context} \n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# 단계 7: 언어모델(LLM) 생성\n",
    "# 모델(LLM) 을 생성합니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# 단계 8: 체인(Chain) 생성\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"chat_history\": itemgetter(\"chat_history\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세션 기록을 저장할 딕셔너리\n",
    "store = {}\n",
    "\n",
    "\n",
    "# 세션 ID를 기반으로 세션 기록을 가져오는 함수\n",
    "def get_session_history(session_ids):\n",
    "    print(f\"[대화 세션ID]: {session_ids}\")\n",
    "    if session_ids not in store:  # 세션 ID가 store에 없는 경우\n",
    "        # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\n",
    "        store[session_ids] = ChatMessageHistory()\n",
    "    return store[session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\n",
    "\n",
    "\n",
    "# 대화를 기록하는 RAG 체인 생성\n",
    "rag_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,  # 세션 기록을 가져오는 함수\n",
    "    input_messages_key=\"question\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\n",
    "    history_messages_key=\"chat_history\",  # 기록 메시지의 키\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션ID]: rag123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"삼성전자가 만든 생성형 AI의 이름은 '삼성 가우스'입니다.\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_with_history.invoke(\n",
    "    # 질문 입력\n",
    "    {\"question\": \"삼성전자가 만든 생성형 AI 이름은?\"},\n",
    "    # 세션 ID 기준으로 대화를 기록합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"rag123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션ID]: rag123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The name of the generative AI created by Samsung Electronics is \"Samsung Gauss.\"'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_with_history.invoke(\n",
    "    # 질문 입력\n",
    "    {\"question\": \"이전 답변을 영어로 번역해주세요.\"},\n",
    "    # 세션 ID 기준으로 대화를 기록합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"rag123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import textwrap\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# Load some data to summarize\n",
    "loader = WebBaseLoader(\"https://teddylee777.github.io/data-science/optuna/\")\n",
    "docs = loader.load()\n",
    "content = docs[0].page_content\n",
    "\n",
    "# Get this prompt template\n",
    "prompt = hub.pull(\"lawwu/chain_of_density\")\n",
    "\n",
    "# The chat model output is a JSON list of dicts, with SimpleJsonOutputParser\n",
    "# we can convert it o a dict, and it suppors streaming.\n",
    "json_parser = SimpleJsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"ARTICLE\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0.1)\n",
    "    | json_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Missing_Entities': 'Optuna',\n",
       "  'Denser_Summary': 'This article discusses Optuna, a hyperparameter optimization library for machine learning. Optuna is a powerful tool that can be used to optimize various parameters in machine learning models. It provides a wide range of functions and methods for suggesting optimal values for parameters such as learning rate, number of layers, and activation functions. With Optuna, you can easily find the best hyperparameters for your models and improve their performance.'},\n",
       " {'Missing_Entities': 'trial.suggest_categorical()',\n",
       "  'Denser_Summary': 'Optuna provides the trial.suggest_categorical() function, which allows you to suggest categorical values for hyperparameters. This function takes a name and a list of choices as input, and Optuna will suggest the best value from the given choices. By using trial.suggest_categorical(), you can easily explore different combinations of categorical hyperparameters and find the optimal configuration for your model.'},\n",
       " {'Missing_Entities': 'trial.suggest_int()',\n",
       "  'Denser_Summary': 'Another useful function provided by Optuna is trial.suggest_int(), which allows you to suggest integer values for hyperparameters. This function takes a name, a lower bound, an upper bound, and an optional step size as input. Optuna will then suggest the best integer value within the specified range. With trial.suggest_int(), you can efficiently search for the optimal integer hyperparameter values for your model.'},\n",
       " {'Missing_Entities': 'trial.suggest_float()',\n",
       "  'Denser_Summary': 'In addition to suggesting categorical and integer values, Optuna also provides the trial.suggest_float() function for suggesting floating-point values. This function takes a name, a lower bound, an upper bound, and an optional step size as input. Optuna will suggest the best floating-point value within the specified range. By using trial.suggest_float(), you can easily explore different ranges of floating-point hyperparameters and find the optimal values for your model.'},\n",
       " {'Missing_Entities': 'objective function',\n",
       "  'Denser_Summary': 'The objective function is a key component in Optuna. It is the function that you define to evaluate the performance of your model based on the suggested hyperparameters. The objective function takes a trial object, a model, input data, output data, and an evaluation metric as input. It trains the model with the suggested hyperparameters, evaluates its performance using the evaluation metric, and returns the error or loss value. By optimizing the objective function, Optuna can find the best hyperparameters for your model.'}]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "    {\n",
      "        \"Missing_Entities\": \"\",\n",
      "        \"Denser_Summary\": \"이 기사는 데이터사이언스, 머신러닝, 인공지능에 대한 개념을 설명하고 있습니다. 이 분야들이 우리 생활에 어떻게 적용되고 있는지, 그리고 이 분야를 공부하고자 하는 사람들이 어떻게 시작해야 할지에 대한 정보를 제공합니다. 또한, 이 기술들이 어떻게 빠르게 발전하고 있는지에 대한 개요를 제공하며, 이 분야의 전문가 인터뽀를 통해 실제 적용 사례와 조언을 들어볼 수 있는 기회를 제공합니다. 이러한 기술들이 어떻게 서로 연관되어 있는지, 그리고 각각의 기술이 어떻게 다른지에 대한 설명도 포함되어 있습니다.\"\n",
      "    },\n",
      "    {\n",
      "        \"Missing_Entities\": \"박정현 서울대 EPM 연구원; 데이터사이언스 대학원과 인공지능 대학원 설립; 지도학습과 비지도 학습\",\n",
      "        \"Denser_Summary\": \"박정현 서울대 EPM 연구원이 작성한 이 기사는 데이터사이언스, 머신러닝, 인공지능의 기본 개념과 실생활 적용 사례를 설명합니다. 대학에서 데이터사이언스와 인공지능 대학원을 설립하는 추세와 이 분야의 전문가 인터뷰를 통한 조언을 포함하고 있습니다. 또한, 머신러닝의 지도학습과 비지도 학습 방식의 차이점에 대해서도 설명하며, 이 기술들이 어떻게 서로 연결되어 있는지를 다룹니다.\"\n",
      "    },\n",
      "    {\n",
      "        \"Missing_Entities\": \"알파고; GPT-3; 튜링테스트; k-means 알고리즘\",\n",
      "        \"Denser_Summary\": \"박정현 서울대 EPM 연구원이 작성한 이 기사는 데이터사이언스, 머신러닝, 인공지능의 정의와 알파고, GPT-3 같은 대형 프로젝트 사례를 소개합니다. 대학의 데이터사이언스와 인공지능 대학원 설립 추세, 지도학습과 비지도 학습의 차이, 튜링테스트를 통한 인공지능의 이해, k-means와 같은 알고리즘 설명을 포함합니다.\"\n",
      "    },\n",
      "    {\n",
      "        \"Missing_Entities\": \"데이터 종류 및 관련 용어; 데이터 분석 문제; 데이터 피처 엔지니어링; 알고리즘 - 회귀, 분류, 클러스터링; 성능평가\",\n",
      "        \"Denser_Summary\": \"박정현 서울대 EPM 연구원이 작성한 기사는 데이터사이언스, 머신러닝, 인공지능의 정의, 알파고, GPT-3 사례, 데이터사이언스 대학원 설립 추세, 지도학습과 비지도 학습 차이, 튜링테스트, k-means 알고리즘을 다룹니다. 추가로 데이터 종류, 분석 문제, 피처 엔지니어링, 회귀/분류/클러스터링 알고리즘, 성능평가 방법을 설명합니다.\"\n",
      "    },\n",
      "    {\n",
      "        \"Missing_Entities\": \"데이터사이언스가 인공지능, 머신러닝보다 큰 범위; 신용카드 결제 데이터에서 사기 거래 검출; 강화학습\",\n",
      "        \"Denser_Summary\": \"박정현 서울대 EPM 연구원이 작성한 기사는 데이터사이언스, 머신러닝, 인공지능의 정의, 알파고, GPT-3 사례, 데이터사이언스 대학원 설립 추세, 지도학습과 비지도 학습 차이, 튜링테스트, k-means 알고리즘을 다룹니다. 데이터 종류, 분석 문제, 피처 엔지니어링, 회귀/분류/클러스터링 알고리즘, 성능평가 방법, 데이터사이언스의 범위, 신용카드 사기 거래 검출, 강화학습에 대해 설명합니다.\"\n",
      "    }\n",
      "]\n",
      "```박정현 서울대 EPM 연구원이 작성한 기사는 데이터사이언스, 머신러닝, 인공지능의 정의, 알파고, GPT-3 사례, 데이터사이언스 대학원 설립 추세, 지도학습과 비지도 학습 차이, 튜링테스트, k-means 알고리즘을 다룹니다. 데이터 종류, 분석 문제, 피처 엔지니어링, 회귀/분류/클러스터링 알고리즘, 성능평가 방법, 데이터사이언스의 범위, 신용카드 사기 거래 검출, 강화학습에 대해 설명합니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "import json\n",
    "\n",
    "\n",
    "# Load some data to summarize\n",
    "loader = WebBaseLoader(\n",
    "    \"https://www.aitimes.com/news/articleView.html?idxno=131777\")\n",
    "docs = loader.load()\n",
    "content = docs[0].page_content\n",
    "# Load the prompt\n",
    "# prompt = hub.pull(\"langchain-ai/chain-of-density:4f55305e\")\n",
    "\n",
    "\n",
    "class StreamCallback(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token, **kwargs):\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Article: {ARTICLE}\n",
    "You will generate increasingly concise, entity-dense summaries of the above article. \n",
    "\n",
    "Repeat the following 2 steps 5 times. \n",
    "\n",
    "Step 1. Identify 1-3 informative entities (\";\" delimited) from the article which are missing from the previously generated summary. \n",
    "Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the missing entities. \n",
    "\n",
    "A missing entity is:\n",
    "- relevant to the main story, \n",
    "- specific yet concise (50 words or fewer), \n",
    "- novel (not in the previous summary), \n",
    "- faithful (present in the article), \n",
    "- anywhere (can be located anywhere in the article).\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "- The first summary should be long (8-10 sentences, ~200 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~200 words.\n",
    "- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n",
    "- Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n",
    "- The summaries should become highly dense and concise yet self-contained, i.e., easily understood without the article. \n",
    "- Missing entities can appear anywhere in the new summary.\n",
    "- Never drop entities from the previous summary. If space cannot be made, add fewer new entities. \n",
    "\n",
    "Remember, use the exact same number of words for each summary.\n",
    "Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\".\n",
    "Use only KOREAN language to reply.\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create the chain, including\n",
    "chain = (\n",
    "    prompt\n",
    "    | ChatOpenAI(\n",
    "        temperature=0,\n",
    "        model=\"gpt-4-turbo-preview\",\n",
    "        streaming=True,\n",
    "        callbacks=[StreamCallback()],\n",
    "    )\n",
    "    | JsonOutputParser()\n",
    "    | (lambda x: x[-1][\"Denser_Summary\"])\n",
    ")\n",
    "\n",
    "# Invoke the chain\n",
    "result = chain.invoke({\"ARTICLE\": content})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain umap-learn scikit-learn langchain_community tiktoken langchain-openai langchainhub chromadb langchain-anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    # 주어진 문자열에서 토큰의 개수를 반환합니다.\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "# LCEL 문서 로드\n",
    "url = \"https://python.langchain.com/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# PydanticOutputParser를 사용한 LCEL 문서 로드 (기본 LCEL 문서 외부)\n",
    "url = \"https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_pydantic = loader.load()\n",
    "\n",
    "# Self Query를 사용한 LCEL 문서 로드 (기본 LCEL 문서 외부)\n",
    "url = \"https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_sq = loader.load()\n",
    "\n",
    "# 문서 텍스트\n",
    "docs.extend([*docs_pydantic, *docs_sq])\n",
    "docs_texts = [d.page_content for d in docs]\n",
    "\n",
    "# 각 문서에 대한 토큰 수 계산\n",
    "counts = [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]\n",
    "\n",
    "# 토큰 수의 히스토그램을 그립니다.\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Token Counts in LCEL Documents\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "\n",
    "# 히스토그램을 표시합니다.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens in all context: 5175\n"
     ]
    }
   ],
   "source": [
    "# 문서 텍스트를 연결합니다.\n",
    "# 문서를 출처 메타데이터 기준으로 정렬합니다.\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))  # 정렬된 문서를 역순으로 배열합니다.\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [\n",
    "        # 역순으로 배열된 문서의 내용을 연결합니다.\n",
    "        doc.page_content\n",
    "        for doc in d_reversed\n",
    "    ]\n",
    ")\n",
    "print(\n",
    "    \"Num tokens in all context: %s\"  # 모든 문맥에서의 토큰 수를 출력합니다.\n",
    "    % num_tokens_from_string(concatenated_content, \"cl100k_base\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 분할을 위한 코드\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size_tok = 2000  # 토큰의 청크 크기를 설정합니다.\n",
    "# 재귀적 문자 텍스트 분할기를 초기화합니다. 토큰 인코더를 사용하여 청크 크기와 중복을 설정합니다.\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=chunk_size_tok, chunk_overlap=0\n",
    ")\n",
    "texts_split = text_splitter.split_text(\n",
    "    concatenated_content\n",
    ")  # 주어진 텍스트를 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "store = LocalFileStore(\"./cache/\")\n",
    "\n",
    "# embeddings 인스턴스를 생성합니다.\n",
    "embd = OpenAIEmbeddings(model=\"text-embedding-3-small\", disallowed_special=())\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embd, store, namespace=embd.model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "\n",
    "class StreamCallback(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "# ChatOpenAI 모델을 초기화합니다. 모델은 \"gpt-4-turbo-preview\"를 사용합니다.\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    "    temperature=0,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamCallback()],\n",
    ")\n",
    "\n",
    "# ChatAnthropic 모델을 초기화합니다. 온도는 0으로 설정하고, 모델은 \"claude-3-opus-20240229\"를 사용합니다.\n",
    "# model = ChatAnthropic(temperature=0, model=\"claude-3-opus-20240229\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "RANDOM_SEED = 42  # 재현성을 위한 고정된 시드 값\n",
    "\n",
    "### --- 위의 인용된 코드에서 주석과 문서화를 추가함 --- ###\n",
    "\n",
    "\n",
    "def global_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    UMAP을 사용하여 임베딩의 전역 차원 축소를 수행합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - embeddings: numpy 배열로 된 입력 임베딩.\n",
    "    - dim: 축소된 공간의 목표 차원.\n",
    "    - n_neighbors: 선택 사항; 각 점을 고려할 이웃의 수.\n",
    "                   제공되지 않으면 임베딩 수의 제곱근으로 기본 설정됩니다.\n",
    "    - metric: UMAP에 사용할 거리 측정 기준.\n",
    "\n",
    "    반환값:\n",
    "    - 지정된 차원으로 축소된 임베딩의 numpy 배열.\n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_cluster_embeddings(\n",
    "    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    임베딩에 대해 지역 차원 축소를 수행합니다. 이는 일반적으로 전역 클러스터링 이후에 사용됩니다.\n",
    "\n",
    "    매개변수:\n",
    "    - embeddings: numpy 배열로서의 입력 임베딩.\n",
    "    - dim: 축소된 공간의 목표 차원 수.\n",
    "    - num_neighbors: 각 점에 대해 고려할 이웃의 수.\n",
    "    - metric: UMAP에 사용할 거리 측정 기준.\n",
    "\n",
    "    반환값:\n",
    "    - 지정된 차원으로 축소된 임베딩의 numpy 배열.\n",
    "    \"\"\"\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_clusters(\n",
    "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    가우시안 혼합 모델(Gaussian Mixture Model)을 사용하여 베이지안 정보 기준(BIC)을 통해 최적의 클러스터 수를 결정합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - embeddings: numpy 배열로서의 입력 임베딩.\n",
    "    - max_clusters: 고려할 최대 클러스터 수.\n",
    "    - random_state: 재현성을 위한 시드.\n",
    "\n",
    "    반환값:\n",
    "    - 발견된 최적의 클러스터 수를 나타내는 정수.\n",
    "    \"\"\"\n",
    "    max_clusters = min(\n",
    "        max_clusters, len(embeddings)\n",
    "    )  # 최대 클러스터 수와 임베딩의 길이 중 작은 값을 최대 클러스터 수로 설정\n",
    "    n_clusters = np.arange(1, max_clusters)  # 1부터 최대 클러스터 수까지의 범위를 생성\n",
    "    bics = []  # BIC 점수를 저장할 리스트\n",
    "    for n in n_clusters:  # 각 클러스터 수에 대해 반복\n",
    "        gm = GaussianMixture(\n",
    "            n_components=n, random_state=random_state\n",
    "        )  # 가우시안 혼합 모델 초기화\n",
    "        gm.fit(embeddings)  # 임베딩에 대해 모델 학습\n",
    "        bics.append(gm.bic(embeddings))  # 학습된 모델의 BIC 점수를 리스트에 추가\n",
    "    return n_clusters[np.argmin(bics)]  # BIC 점수가 가장 낮은 클러스터 수를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    \"\"\"\n",
    "    확률 임계값을 기반으로 가우시안 혼합 모델(GMM)을 사용하여 임베딩을 클러스터링합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - embeddings: numpy 배열로서의 입력 임베딩.\n",
    "    - threshold: 임베딩을 클러스터에 할당하기 위한 확률 임계값.\n",
    "    - random_state: 재현성을 위한 시드.\n",
    "\n",
    "    반환값:\n",
    "    - 클러스터 레이블과 결정된 클러스터 수를 포함하는 튜플.\n",
    "    \"\"\"\n",
    "    n_clusters = get_optimal_clusters(embeddings)  # 최적의 클러스터 수를 구합니다.\n",
    "    # 가우시안 혼합 모델을 초기화합니다.\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)  # 임베딩에 대해 모델을 학습합니다.\n",
    "    probs = gm.predict_proba(\n",
    "        embeddings\n",
    "    )  # 임베딩이 각 클러스터에 속할 확률을 예측합니다.\n",
    "    # 임계값을 초과하는 확률을 가진 클러스터를 레이블로 선택합니다.\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters  # 레이블과 클러스터 수를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    threshold: float,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    임베딩에 대해 차원 축소, 가우시안 혼합 모델을 사용한 클러스터링, 각 글로벌 클러스터 내에서의 로컬 클러스터링을 순서대로 수행합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - embeddings: numpy 배열로 된 입력 임베딩입니다.\n",
    "    - dim: UMAP 축소를 위한 목표 차원입니다.\n",
    "    - threshold: GMM에서 임베딩을 클러스터에 할당하기 위한 확률 임계값입니다.\n",
    "\n",
    "    반환값:\n",
    "    - 각 임베딩의 클러스터 ID를 포함하는 numpy 배열의 리스트입니다.\n",
    "    \"\"\"\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        # 데이터가 충분하지 않을 때 클러스터링을 피합니다.\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "\n",
    "    # 글로벌 차원 축소\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    # 글로벌 클러스터링\n",
    "    global_clusters, n_global_clusters = GMM_cluster(\n",
    "        reduced_embeddings_global, threshold\n",
    "    )\n",
    "\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    # 각 글로벌 클러스터를 순회하며 로컬 클러스터링 수행\n",
    "    for i in range(n_global_clusters):\n",
    "        # 현재 글로벌 클러스터에 속하는 임베딩 추출\n",
    "        global_cluster_embeddings_ = embeddings[\n",
    "            np.array([i in gc for gc in global_clusters])\n",
    "        ]\n",
    "\n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            # 작은 클러스터는 직접 할당으로 처리\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters = 1\n",
    "        else:\n",
    "            # 로컬 차원 축소 및 클러스터링\n",
    "            reduced_embeddings_local = local_cluster_embeddings(\n",
    "                global_cluster_embeddings_, dim\n",
    "            )\n",
    "            local_clusters, n_local_clusters = GMM_cluster(\n",
    "                reduced_embeddings_local, threshold\n",
    "            )\n",
    "\n",
    "        # 로컬 클러스터 ID 할당, 이미 처리된 총 클러스터 수를 조정\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(\n",
    "                    all_local_clusters[idx], j + total_clusters\n",
    "                )\n",
    "\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(texts):\n",
    "    # 텍스트 문서 목록에 대한 임베딩을 생성합니다.\n",
    "    #\n",
    "    # 이 함수는 `embd` 객체가 존재한다고 가정하며, 이 객체는 텍스트 목록을 받아 그 임베딩을 반환하는 `embed_documents` 메소드를 가지고 있습니다.\n",
    "    #\n",
    "    # 매개변수:\n",
    "    # - texts: List[str], 임베딩할 텍스트 문서의 목록입니다.\n",
    "    #\n",
    "    # 반환값:\n",
    "    # - numpy.ndarray: 주어진 텍스트 문서들에 대한 임베딩 배열입니다.\n",
    "    text_embeddings = embd.embed_documents(\n",
    "        texts\n",
    "    )  # 텍스트 문서들의 임베딩을 생성합니다.\n",
    "    text_embeddings_np = np.array(text_embeddings)  # 임베딩을 numpy 배열로 변환합니다.\n",
    "    return text_embeddings_np  # 임베딩된 numpy 배열을 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_cluster_texts(texts):\n",
    "    \"\"\"\n",
    "    텍스트 목록을 임베딩하고 클러스터링하여, 텍스트, 그들의 임베딩, 그리고 클러스터 라벨이 포함된 DataFrame을 반환합니다.\n",
    "\n",
    "    이 함수는 임베딩 생성과 클러스터링을 단일 단계로 결합합니다. 임베딩에 대해 클러스터링을 수행하는 `perform_clustering` 함수의 사전 정의된 존재를 가정합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - texts: List[str], 처리될 텍스트 문서의 목록입니다.\n",
    "\n",
    "    반환값:\n",
    "    - pandas.DataFrame: 원본 텍스트, 그들의 임베딩, 그리고 할당된 클러스터 라벨이 포함된 DataFrame입니다.\n",
    "    \"\"\"\n",
    "    text_embeddings_np = embed(texts)  # 임베딩 생성\n",
    "    cluster_labels = perform_clustering(\n",
    "        text_embeddings_np, 10, 0.1\n",
    "    )  # 임베딩에 대해 클러스터링 수행\n",
    "    df = pd.DataFrame()  # 결과를 저장할 DataFrame 초기화\n",
    "    df[\"text\"] = texts  # 원본 텍스트 저장\n",
    "    df[\"embd\"] = list(text_embeddings_np)  # DataFrame에 리스트로 임베딩 저장\n",
    "    df[\"cluster\"] = cluster_labels  # 클러스터 라벨 저장\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_txt(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    DataFrame에 있는 텍스트 문서를 단일 문자열로 포맷합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - df: 'text' 열에 포맷할 텍스트 문서가 포함된 DataFrame.\n",
    "\n",
    "    반환값:\n",
    "    - 모든 텍스트 문서가 특정 구분자로 결합된 단일 문자열.\n",
    "    \"\"\"\n",
    "    unique_txt = df[\"text\"].tolist()  # 'text' 열의 모든 텍스트를 리스트로 변환\n",
    "    return \"--- --- \\n --- --- \".join(\n",
    "        unique_txt\n",
    "    )  # 텍스트 문서들을 특정 구분자로 결합하여 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_cluster_summarize_texts(\n",
    "    texts: List[str], level: int\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    텍스트 목록에 대해 임베딩, 클러스터링 및 요약을 수행합니다. 이 함수는 먼저 텍스트에 대한 임베딩을 생성하고,\n",
    "    유사성을 기반으로 클러스터링을 수행한 다음, 클러스터 할당을 확장하여 처리를 용이하게 하고 각 클러스터 내의 내용을 요약합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - texts: 처리할 텍스트 문서 목록입니다.\n",
    "    - level: 처리의 깊이나 세부 사항을 정의할 수 있는 정수 매개변수입니다.\n",
    "\n",
    "    반환값:\n",
    "    - 두 개의 데이터프레임을 포함하는 튜플:\n",
    "      1. 첫 번째 데이터프레임(`df_clusters`)은 원본 텍스트, 그들의 임베딩, 그리고 클러스터 할당을 포함합니다.\n",
    "      2. 두 번째 데이터프레임(`df_summary`)은 각 클러스터에 대한 요약, 지정된 세부 수준, 그리고 클러스터 식별자를 포함합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    # 텍스트를 임베딩하고 클러스터링하여 'text', 'embd', 'cluster' 열이 있는 데이터프레임을 생성합니다.\n",
    "    df_clusters = embed_cluster_texts(texts)\n",
    "\n",
    "    # 클러스터를 쉽게 조작하기 위해 데이터프레임을 확장할 준비를 합니다.\n",
    "    expanded_list = []\n",
    "\n",
    "    # 데이터프레임 항목을 문서-클러스터 쌍으로 확장하여 처리를 간단하게 합니다.\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row[\"cluster\"]:\n",
    "            expanded_list.append(\n",
    "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "            )\n",
    "\n",
    "    # 확장된 목록에서 새 데이터프레임을 생성합니다.\n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "\n",
    "    # 처리를 위해 고유한 클러스터 식별자를 검색합니다.\n",
    "    all_clusters = expanded_df[\"cluster\"].unique()\n",
    "\n",
    "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
    "\n",
    "    # 요약\n",
    "    template = \"\"\"여기 LangChain 표현 언어 문서의 하위 집합이 있습니다.\n",
    "    \n",
    "    LangChain 표현 언어는 LangChain에서 체인을 구성하는 방법을 제공합니다.\n",
    "    \n",
    "    제공된 문서의 자세한 요약을 제공하십시오.\n",
    "    \n",
    "    문서:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "\n",
    "    # 각 클러스터 내의 텍스트를 요약을 위해 포맷팅합니다.\n",
    "    summaries = []\n",
    "    for i in all_clusters:\n",
    "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "        formatted_txt = fmt_txt(df_cluster)\n",
    "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    "\n",
    "    # 요약, 해당 클러스터 및 레벨을 저장할 데이터프레임을 생성합니다.\n",
    "    df_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"summaries\": summaries,\n",
    "            \"level\": [level] * len(summaries),\n",
    "            \"cluster\": list(all_clusters),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_clusters, df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_embed_cluster_summarize(\n",
    "    texts: List[str], level: int = 1, n_levels: int = 3\n",
    ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    지정된 레벨까지 또는 고유 클러스터의 수가 1이 될 때까지 텍스트를 재귀적으로 임베딩, 클러스터링, 요약하여\n",
    "    각 레벨에서의 결과를 저장합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - texts: List[str], 처리할 텍스트들.\n",
    "    - level: int, 현재 재귀 레벨 (1에서 시작).\n",
    "    - n_levels: int, 재귀의 최대 깊이.\n",
    "\n",
    "    반환값:\n",
    "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], 재귀 레벨을 키로 하고 해당 레벨에서의 클러스터 DataFrame과 요약 DataFrame을 포함하는 튜플을 값으로 하는 사전.\n",
    "    \"\"\"\n",
    "    results = {}  # 각 레벨에서의 결과를 저장할 사전\n",
    "\n",
    "    # 현재 레벨에 대해 임베딩, 클러스터링, 요약 수행\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
    "\n",
    "    # 현재 레벨의 결과 저장\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # 추가 재귀가 가능하고 의미가 있는지 결정\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # 다음 레벨의 재귀 입력 텍스트로 요약 사용\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(\n",
    "            new_texts, level + 1, n_levels\n",
    "        )\n",
    "\n",
    "        # 다음 레벨의 결과를 현재 결과 사전에 병합\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 문서의 개수\n",
    "len(docs_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generated 1 clusters--\n",
      "LangChain Expression Language (LCEL)는 LangChain에서 체인을 구성하는 선언적 방법을 제공합니다. LCEL은 프로토타입을 생산에 투입하는 것을 지원하도록 설계되었으며, 가장 간단한 \"프롬프트 + LLM\" 체인부터 수백 단계의 복잡한 체인까지 코드 변경 없이 생산에 투입할 수 있습니다. LCEL을 사용하는 이유 중 몇 가지는 다음과 같습니다:\n",
      "\n",
      "- **스트리밍 지원**: LCEL로 체인을 구축하면 최상의 첫 번째 토큰 시간(첫 번째 출력 청크가 나올 때까지의 시간)을 얻을 수 있습니다. 일부 체인의 경우, LLM에서 스트리밍 출력 파서로 토큰을 스트리밍하고, LLM 제공자가 원시 토큰을 출력하는 속도로 파싱된 증분 청크를 받을 수 있습니다.\n",
      "- **비동기 지원**: LCEL로 구축된 모든 체인은 동기 API(예: 프로토타이핑 중인 Jupyter 노트북에서)와 비동기 API(예: LangServe 서버에서) 모두에서 호출할 수 있습니다. 이를 통해 동일한 코드를 프로토타입과 생산에서 사용할 수 있으며, 동시에 많은 요청을 처리할 수 있습니다.\n",
      "- **병렬 실행 최적화**: LCEL 체인에 병렬로 실행할 수 있는 단계가 있는 경우(예: 여러 검색기에서 문서를 가져올 때), 동기 및 비동기 인터페이스 모두에서 자동으로 병렬 실행을 수행하여 가능한 가장 작은 대기 시간을 달성합니다.\n",
      "- **재시도 및 대체**: LCEL 체인의 모든 부분에 대해 재시도 및 대체를 구성할 수 있습니다. 이는 규모에 따른 체인의 신뢰성을 높이는 좋은 방법입니다.\n",
      "- **중간 결과 접근**: 더 복잡한 체인의 경우, 최종 출력이 생성되기 전에 중간 단계의 결과에 접근하는 것이 매우 유용할 수 있습니다. 이를 통해 사용자에게 무언가가 진행되고 있음을 알리거나 체인을 디버깅할 수 있습니다.\n",
      "- **입력 및 출력 스키마**: 모든 LCEL 체인은 체인의 구조에서 추론된 Pydantic 및 JSONSchema 스키마를 갖습니다. 이는 입력 및 출력의 유효성 검사에 사용될 수 있습니다.\n",
      "- **LangSmith 추적 및 LangServe 배포**: 체인이 점점 더 복잡해짐에 따라 각 단계에서 정확히 무슨 일이 일어나고 있는지 이해하는 것이 점점 더 중요해집니다. LCEL을 사용하면 모든 단계가 LangSmith에 자동으로 로그되어 최대한의 관찰 가능성과 디버깅 가능성을 제공합니다. 또한, LCEL로 생성된 모든 체인은 LangServe를 사용하여 쉽게 배포할 수 있습니다.\n",
      "\n",
      "LCEL은 또한 출력 파서를 포함하여 언어 모델 응답을 더 구조화된 정보로 변환하는 클래스를 제공합니다. 출력 파서는 \"얻기 형식 지시문\"과 \"파싱\"이라는 두 가지 주요 메소드를 구현해야 합니다. 선택적으로 \"프롬프트와 함께 파싱\"이라는 메소드도 구현할 수 있습니다. 이러한 출력 파서는 LangChain 표현 언어(LCEL)의 기본 구성 요소인 Runnable 인터페이스를 구현합니다.\n",
      "\n",
      "또한, 자체 쿼리 검색기는 자연어 쿼리를 받아 구조화된 쿼리를 작성하고 이를 기반으로 문서의 내용과 메타데이터에 대한 필터를 적용하여 검색을 수행할 수 있는 능력을 가집니다. 이를 통해 사용자 입력 쿼리를 사용하여 저장된 문서의 내용과의 의미적 유사성 비교뿐만 아니라 사용자 쿼리에서 문서의 메타데이터에 대한 필터를 추출하고 실행할 수 있습니다.\n",
      "\n",
      "이 문서는 LangChain을 사용하여 복잡한 체인을 구성하고 최적화하는 방법에 대한 깊은 이해를 제공합니다."
     ]
    }
   ],
   "source": [
    "# 트리 구축\n",
    "leaf_texts = docs_texts  # 문서 텍스트를 리프 텍스트로 설정\n",
    "results = recursive_embed_cluster_summarize(\n",
    "    leaf_texts, level=1, n_levels=3\n",
    ")  # 재귀적으로 임베딩, 클러스터링 및 요약을 수행하여 결과를 얻음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# leaf_texts를 복사하여 all_texts를 초기화합니다.\n",
    "all_texts = leaf_texts.copy()\n",
    "\n",
    "# 각 레벨의 요약을 추출하여 all_texts에 추가하기 위해 결과를 순회합니다.\n",
    "for level in sorted(results.keys()):\n",
    "    # 현재 레벨의 DataFrame에서 요약을 추출합니다.\n",
    "    summaries = results[level][1][\"summaries\"].tolist()\n",
    "    # 현재 레벨의 요약을 all_texts에 추가합니다.\n",
    "    all_texts.extend(summaries)\n",
    "\n",
    "# 이제 all_texts를 사용하여 FAISS vectorstore를 구축합니다.\n",
    "vectorstore = FAISS.from_texts(texts=all_texts, embedding=embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DB_INDEX = \"RAPTOR\"\n",
    "\n",
    "# 로컬에 FAISS DB 인덱스가 이미 존재하는지 확인하고, 그렇다면 로드하여 vectorstore와 병합한 후 저장합니다.\n",
    "if os.path.exists(DB_INDEX):\n",
    "    local_index = FAISS.load_local(DB_INDEX, embd)\n",
    "    local_index.merge_from(vectorstore)\n",
    "    local_index.save_local(DB_INDEX)\n",
    "else:\n",
    "    vectorstore.save_local(folder_path=DB_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever 생성\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 프롬프트 생성\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# 문서 포스트 프로세싱\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    # 문서의 페이지 내용을 이어붙여 반환합니다.\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# RAG 체인 정의\n",
    "rag_chain = (\n",
    "    # 검색 결과를 포맷팅하고 질문을 처리합니다.\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt  # 프롬프트를 적용합니다.\n",
    "    | model  # 모델을 적용합니다.\n",
    "    | StrOutputParser()  # 문자열 출력 파서를 적용합니다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 문서는 LangChain Expression Language (LCEL)를 사용하여 복잡한 체인을 구성하고 최적화하는 방법에 대해 설명합니다. LCEL은 프로토타입을 생산에 투입하는 것을 지원하며, 스트리밍 지원, 비동기 지원, 병렬 실행 최적화 등 다양한 기능을 제공합니다. 또한, 출력 파서를 포함하여 언어 모델 응답을 더 구조화된 정보로 변환하는 클래스와 자체 쿼리 검색기를 제공합니다."
     ]
    }
   ],
   "source": [
    "# 추상적인 질문 실행\n",
    "_ = rag_chain.invoke(\"전체 문서의 핵심 주제에 대해 설명해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from langchain.output_parsers import PydanticOutputParser\n",
      "from langchain_core.prompts import PromptTemplate\n",
      "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
      "from langchain_openai import OpenAI\n",
      "\n",
      "# Define your desired data structure.\n",
      "class Joke(BaseModel):\n",
      "    setup: str = Field(description=\"question to set up a joke\")\n",
      "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
      "    # You can add custom validation logic easily with Pydantic.\n",
      "    @validator(\"setup\")\n",
      "    def question_ends_with_question_mark(cls, field):\n",
      "        if field[-1] != \"?\":\n",
      "            raise ValueError(\"Badly formed question!\")\n",
      "        return field\n",
      "\n",
      "# Set up a parser + inject instructions into the prompt template.\n",
      "parser = PydanticOutputParser(pydantic_object=Joke)\n",
      "prompt = PromptTemplate(\n",
      "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
      "    input_variables=[\"query\"],\n",
      "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
      ")\n",
      "\n",
      "# And a query intended to prompt a language model to populate the data structure.\n",
      "model = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)\n",
      "prompt_and_model = prompt | model\n",
      "output = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})\n",
      "parser.invoke(output)\n",
      "```"
     ]
    }
   ],
   "source": [
    "# Low Level 질문 실행\n",
    "_ = rag_chain.invoke(\"PydanticOutputParser 을 활용한 예시 코드를 작성해 주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-querying 방법은 자연어 쿼리를 받아 구조화된 쿼리를 작성하고, 이를 기반으로 문서의 내용과 메타데이터에 대한 필터를 적용하여 검색을 수행하는 방식입니다. 예를 들어, Chroma vector store를 사용하여 영화 요약 문서 집합에 대한 자체 쿼리 검색기를 구현할 수 있습니다. 예시 코드는 다음과 같습니다:\n",
      "\n",
      "```python\n",
      "from langchain_chroma import Chroma\n",
      "from langchain_core.documents import Document\n",
      "from langchain_openai import OpenAIEmbeddings\n",
      "from langchain.chains.query_constructor.base import AttributeInfo\n",
      "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "docs = [\n",
      "    Document(page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\", metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"}),\n",
      "    # 여기에 더 많은 문서 추가...\n",
      "]\n",
      "\n",
      "vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())\n",
      "metadata_field_info = [\n",
      "    AttributeInfo(name=\"genre\", description=\"The genre of the movie.\", type=\"string\"),\n",
      "    AttributeInfo(name=\"year\", description=\"The year the movie was released\", type=\"integer\"),\n",
      "    # 여기에 더 많은 메타데이터 필드 정보 추가...\n",
      "]\n",
      "\n",
      "document_content_description = \"Brief summary of a movie\"\n",
      "llm = ChatOpenAI(temperature=0)\n",
      "retriever = SelfQueryRetriever.from_llm(llm, vectorstore, document_content_description, metadata_field_info)\n",
      "\n",
      "# 검색기 사용 예시\n",
      "retriever.invoke(\"I want to watch a movie rated higher than 8.5\")\n",
      "```\n",
      "\n",
      "이 코드는 영화 요약과 메타데이터를 포함한 문서 집합에서 사용자의 자연어 쿼리를 기반으로 구조화된 쿼리를 생성하고 실행하여 결과를 반환합니다."
     ]
    }
   ],
   "source": [
    "# Low Level 질문 실행\n",
    "_ = rag_chain.invoke(\"self-querying 방법과 예시 코드를 작성해 주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pdf2imag (from versions: none)\n",
      "ERROR: No matching distribution found for pdf2imag\n"
     ]
    }
   ],
   "source": [
    "!pip install pdf2imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U langchain openai chromadb langchain-experimental \n",
    "# # 최신 버전이 필요합니다 (멀티 모달을 위해)\n",
    "# ! pip install \"unstructured[all-docs]\" pillow pydantic lxml pillow matplotlib chromadb tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로\n",
    "fpath = \"data/cj/\"\n",
    "fname = \"cj.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "PDFInfoNotInstalledError",
     "evalue": "Unable to get page count. Is poppler installed and in PATH?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\pdf2image\\pdf2image.py:581\u001b[0m, in \u001b[0;36mpdfinfo_from_path\u001b[1;34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout, first_page, last_page)\u001b[0m\n\u001b[0;32m    580\u001b[0m     env[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLD_LIBRARY_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m poppler_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m env\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLD_LIBRARY_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 581\u001b[0m proc \u001b[38;5;241m=\u001b[39m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\subprocess.py:1538\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mCreateProcess(executable, args,\n\u001b[0;32m   1539\u001b[0m                              \u001b[38;5;66;03m# no special security\u001b[39;00m\n\u001b[0;32m   1540\u001b[0m                              \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1541\u001b[0m                              \u001b[38;5;28mint\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m close_fds),\n\u001b[0;32m   1542\u001b[0m                              creationflags,\n\u001b[0;32m   1543\u001b[0m                              env,\n\u001b[0;32m   1544\u001b[0m                              cwd,\n\u001b[0;32m   1545\u001b[0m                              startupinfo)\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 지정된 파일을 찾을 수 없습니다",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPDFInfoNotInstalledError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[151], line 45\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m texts, tables\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# 요소 추출\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m raw_pdf_elements \u001b[38;5;241m=\u001b[39m \u001b[43mextract_pdf_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# 텍스트, 테이블 추출\u001b[39;00m\n\u001b[0;32m     48\u001b[0m texts, tables \u001b[38;5;241m=\u001b[39m categorize_elements(raw_pdf_elements)\n",
      "Cell \u001b[1;32mIn[151], line 14\u001b[0m, in \u001b[0;36mextract_pdf_elements\u001b[1;34m(path, fname)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_pdf_elements\u001b[39m(path, fname):\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    PDF 파일에서 이미지, 테이블, 그리고 텍스트 조각을 추출합니다.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    path: 이미지(.jpg)를 저장할 파일 경로\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    fname: 파일 이름\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartition_pdf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# PDF 내 이미지 추출 활성화\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 테이블 구조 추론 활성화\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunking_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mby_title\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 제목별로 텍스트 조각화\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_characters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 최대 문자 수\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_after_n_chars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 이 문자 수 이후에 새로운 조각 생성\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcombine_text_under_n_chars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 이 문자 수 이하의 텍스트는 결합\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_output_dir_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 이미지 출력 디렉토리 경로\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\unstructured\\documents\\elements.py:280\u001b[0m, in \u001b[0;36mprocess_metadata.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Element]:\n\u001b[1;32m--> 280\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m     sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(func)\n\u001b[0;32m    282\u001b[0m     params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\unstructured\\file_utils\\filetype.py:551\u001b[0m, in \u001b[0;36madd_metadata_with_filetype.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Element]:\n\u001b[1;32m--> 551\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m     sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(func)\n\u001b[0;32m    553\u001b[0m     params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\unstructured\\chunking\\title.py:257\u001b[0m, in \u001b[0;36madd_chunking_strategy.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Element]:\n\u001b[1;32m--> 257\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m     sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(func)\n\u001b[0;32m    259\u001b[0m     params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\unstructured\\partition\\pdf.py:155\u001b[0m, in \u001b[0;36mpartition_pdf\u001b[1;34m(filename, file, include_page_breaks, strategy, infer_table_structure, ocr_languages, languages, max_partition, min_partition, include_metadata, metadata_filename, metadata_last_modified, chunking_strategy, links, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m         languages \u001b[38;5;241m=\u001b[39m convert_old_ocr_languages_to_languages(ocr_languages)\n\u001b[0;32m    150\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    151\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe ocr_languages kwarg will be deprecated in a future version of unstructured. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use languages instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    153\u001b[0m         )\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartition_pdf_or_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_partition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_partition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\unstructured\\partition\\pdf.py:278\u001b[0m, in \u001b[0;36mpartition_pdf_or_image\u001b[1;34m(filename, file, is_image, include_page_breaks, strategy, infer_table_structure, ocr_languages, languages, max_partition, min_partition, metadata_last_modified, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[0;32m    277\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 278\u001b[0m     _layout_elements \u001b[38;5;241m=\u001b[39m \u001b[43m_partition_pdf_or_image_local\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspooled_to_bytes_io_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mocr_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentire_page\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlast_modification_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m     layout_elements \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m _layout_elements:\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\unstructured\\utils.py:159\u001b[0m, in \u001b[0;36mrequires_dependencies.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_deps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFollowing dependencies are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_deps)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m         ),\n\u001b[0;32m    158\u001b[0m     )\n\u001b[1;32m--> 159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\unstructured\\partition\\pdf.py:363\u001b[0m, in \u001b[0;36m_partition_pdf_or_image_local\u001b[1;34m(filename, file, is_image, infer_table_structure, include_page_breaks, languages, ocr_mode, model_name, metadata_last_modified, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m         process_with_model_kwargs[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m     layout \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_file_with_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprocess_with_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    368\u001b[0m     layout \u001b[38;5;241m=\u001b[39m process_data_with_model(\n\u001b[0;32m    369\u001b[0m         file,\n\u001b[0;32m    370\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprocess_with_model_kwargs,\n\u001b[0;32m    371\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\unstructured_inference\\inference\\layout.py:611\u001b[0m, in \u001b[0;36mprocess_file_with_model\u001b[1;34m(filename, model_name, is_image, ocr_strategy, ocr_languages, ocr_mode, fixed_layouts, extract_tables, pdf_image_dpi, **kwargs)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported model type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    599\u001b[0m layout \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    600\u001b[0m     DocumentLayout\u001b[38;5;241m.\u001b[39mfrom_image_file(\n\u001b[0;32m    601\u001b[0m         filename,\n\u001b[0;32m    602\u001b[0m         detection_model\u001b[38;5;241m=\u001b[39mdetection_model,\n\u001b[0;32m    603\u001b[0m         element_extraction_model\u001b[38;5;241m=\u001b[39melement_extraction_model,\n\u001b[0;32m    604\u001b[0m         ocr_strategy\u001b[38;5;241m=\u001b[39mocr_strategy,\n\u001b[0;32m    605\u001b[0m         ocr_languages\u001b[38;5;241m=\u001b[39mocr_languages,\n\u001b[0;32m    606\u001b[0m         ocr_mode\u001b[38;5;241m=\u001b[39mocr_mode,\n\u001b[0;32m    607\u001b[0m         extract_tables\u001b[38;5;241m=\u001b[39mextract_tables,\n\u001b[0;32m    608\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    609\u001b[0m     )\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_image\n\u001b[1;32m--> 611\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mDocumentLayout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetection_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetection_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43melement_extraction_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43melement_extraction_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mocr_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mocr_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mocr_languages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mocr_languages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mocr_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mocr_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfixed_layouts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextract_tables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpdf_image_dpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpdf_image_dpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m )\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m layout\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\unstructured_inference\\inference\\layout.py:98\u001b[0m, in \u001b[0;36mDocumentLayout.from_file\u001b[1;34m(cls, filename, detection_model, element_extraction_model, fixed_layouts, ocr_strategy, ocr_languages, ocr_mode, extract_tables, pdf_image_dpi, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading PDF for file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory() \u001b[38;5;28;01mas\u001b[39;00m temp_dir:\n\u001b[1;32m---> 98\u001b[0m     layouts, _image_paths \u001b[38;5;241m=\u001b[39m \u001b[43mload_pdf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpdf_image_dpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     image_paths \u001b[38;5;241m=\u001b[39m cast(List[\u001b[38;5;28mstr\u001b[39m], _image_paths)\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layouts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(image_paths):\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\unstructured_inference\\inference\\layout.py:696\u001b[0m, in \u001b[0;36mload_pdf\u001b[1;34m(filename, dpi, output_folder, path_only)\u001b[0m\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_folder must be specified if path_only is true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 696\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43mpdf2image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpaths_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     images \u001b[38;5;241m=\u001b[39m pdf2image\u001b[38;5;241m.\u001b[39mconvert_from_path(\n\u001b[0;32m    704\u001b[0m         filename,\n\u001b[0;32m    705\u001b[0m         dpi\u001b[38;5;241m=\u001b[39mdpi,\n\u001b[0;32m    706\u001b[0m         paths_only\u001b[38;5;241m=\u001b[39mpath_only,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\pdf2image\\pdf2image.py:127\u001b[0m, in \u001b[0;36mconvert_from_path\u001b[1;34m(pdf_path, dpi, output_folder, first_page, last_page, fmt, jpegopt, thread_count, userpw, ownerpw, use_cropbox, strict, transparent, single_file, output_file, poppler_path, grayscale, size, paths_only, use_pdftocairo, timeout, hide_annotations)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(poppler_path, PurePath):\n\u001b[0;32m    125\u001b[0m     poppler_path \u001b[38;5;241m=\u001b[39m poppler_path\u001b[38;5;241m.\u001b[39mas_posix()\n\u001b[1;32m--> 127\u001b[0m page_count \u001b[38;5;241m=\u001b[39m \u001b[43mpdfinfo_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserpw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mownerpw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoppler_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoppler_path\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# We start by getting the output format, the buffer processing function and if we need pdftocairo\u001b[39;00m\n\u001b[0;32m    132\u001b[0m parsed_fmt, final_extension, parse_buffer_func, use_pdfcairo_format \u001b[38;5;241m=\u001b[39m _parse_format(\n\u001b[0;32m    133\u001b[0m     fmt, grayscale\n\u001b[0;32m    134\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\UserK\\Desktop\\LangChain\\langchain_me\\.conda\\Lib\\site-packages\\pdf2image\\pdf2image.py:607\u001b[0m, in \u001b[0;36mpdfinfo_from_path\u001b[1;34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout, first_page, last_page)\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m d\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PDFInfoNotInstalledError(\n\u001b[0;32m    608\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to get page count. Is poppler installed and in PATH?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    609\u001b[0m     )\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PDFPageCountError(\n\u001b[0;32m    612\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to get page count.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m     )\n",
      "\u001b[1;31mPDFInfoNotInstalledError\u001b[0m: Unable to get page count. Is poppler installed and in PATH?"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# PDF에서 요소 추출\n",
    "\n",
    "\n",
    "def extract_pdf_elements(path, fname):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 이미지, 테이블, 그리고 텍스트 조각을 추출합니다.\n",
    "    path: 이미지(.jpg)를 저장할 파일 경로\n",
    "    fname: 파일 이름\n",
    "    \"\"\"\n",
    "    return partition_pdf(\n",
    "        filename=os.path.join(path, fname),\n",
    "        extract_images_in_pdf=True,  # PDF 내 이미지 추출 활성화\n",
    "        infer_table_structure=True,  # 테이블 구조 추론 활성화\n",
    "        chunking_strategy=\"by_title\",  # 제목별로 텍스트 조각화\n",
    "        max_characters=4000,  # 최대 문자 수\n",
    "        new_after_n_chars=3800,  # 이 문자 수 이후에 새로운 조각 생성\n",
    "        combine_text_under_n_chars=2000,  # 이 문자 수 이하의 텍스트는 결합\n",
    "        image_output_dir_path=path,  # 이미지 출력 디렉토리 경로\n",
    "    )\n",
    "\n",
    "\n",
    "# 요소를 유형별로 분류\n",
    "\n",
    "\n",
    "def categorize_elements(raw_pdf_elements):\n",
    "    \"\"\"\n",
    "    PDF에서 추출된 요소를 테이블과 텍스트로 분류합니다.\n",
    "    raw_pdf_elements: unstructured.documents.elements의 리스트\n",
    "    \"\"\"\n",
    "    tables = []  # 테이블 저장 리스트\n",
    "    texts = []  # 텍스트 저장 리스트\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            tables.append(str(element))  # 테이블 요소 추가\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            texts.append(str(element))  # 텍스트 요소 추가\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "# 요소 추출\n",
    "raw_pdf_elements = extract_pdf_elements(fpath, fname)\n",
    "\n",
    "# 텍스트, 테이블 추출\n",
    "texts, tables = categorize_elements(raw_pdf_elements)\n",
    "\n",
    "# 선택사항: 텍스트에 대해 특정 토큰 크기 적용\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=4000, chunk_overlap=0  # 텍스트를 4000 토큰 크기로 분할, 중복 없음\n",
    ")\n",
    "joined_texts = \" \".join(texts)  # 텍스트 결합\n",
    "texts_4k_token = text_splitter.split_text(joined_texts)  # 분할 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts_4k_token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[145], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtexts_4k_token\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'texts_4k_token' is not defined"
     ]
    }
   ],
   "source": [
    "len(texts_4k_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts_4k_token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[152], line 45\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text_summaries, table_summaries\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# 텍스트, 테이블 요약 가져오기\u001b[39;00m\n\u001b[0;32m     44\u001b[0m text_summaries, table_summaries \u001b[38;5;241m=\u001b[39m generate_text_summaries(\n\u001b[1;32m---> 45\u001b[0m     \u001b[43mtexts_4k_token\u001b[49m, tables, summarize_texts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     46\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'texts_4k_token' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 텍스트 요소의 요약 생성\n",
    "\n",
    "\n",
    "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "    \"\"\"\n",
    "    텍스트 요소 요약\n",
    "    texts: 문자열 리스트\n",
    "    tables: 문자열 리스트\n",
    "    summarize_texts: 텍스트 요약 여부를 결정. True/False\n",
    "    \"\"\"\n",
    "\n",
    "    # 프롬프트 설정\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # 텍스트 요약 체인\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # 요약을 위한 빈 리스트 초기화\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # 제공된 텍스트에 대해 요약이 요청되었을 경우 적용\n",
    "    if texts and summarize_texts:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "    # 제공된 테이블에 적용\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "\n",
    "    return text_summaries, table_summaries\n",
    "\n",
    "\n",
    "# 텍스트, 테이블 요약 가져오기\n",
    "text_summaries, table_summaries = generate_text_summaries(\n",
    "    texts_4k_token, tables, summarize_texts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
